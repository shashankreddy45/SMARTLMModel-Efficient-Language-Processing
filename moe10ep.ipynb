{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90aa24c0-af47-4e80-ac99-b4a6d1d39463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "PyTorch CUDA Version: 12.1\n",
      "PennyLane Version: 0.38.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since HuggingFaceTB/cosmopedia couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'khanacademy' at C:\\Users\\saikr\\.cache\\huggingface\\datasets\\HuggingFaceTB___cosmopedia\\khanacademy\\0.0.0\\0ae6ec63f91742bd2d1eaef4f02232c55d719385 (last modified on Tue Jan 28 22:25:22 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 examples...\n",
      "Processed 2000 examples...\n",
      "Processed 3000 examples...\n",
      "Processed 4000 examples...\n",
      "Processed 5000 examples...\n",
      "Processed 6000 examples...\n",
      "Processed 7000 examples...\n",
      "Processed 8000 examples...\n",
      "Processed 9000 examples...\n",
      "Processed 10000 examples...\n",
      "Processed 11000 examples...\n",
      "Processed 12000 examples...\n",
      "Processed 13000 examples...\n",
      "Processed 14000 examples...\n",
      "Processed 15000 examples...\n",
      "Processed 16000 examples...\n",
      "Processed 17000 examples...\n",
      "Processed 18000 examples...\n",
      "Processed 19000 examples...\n",
      "Processed 20000 examples...\n",
      "Processed 21000 examples...\n",
      "Processed 22000 examples...\n",
      "Processed 23000 examples...\n",
      "Processed 24000 examples...\n",
      "Completed writing 24123 examples to cosmopedia_full.txt\n",
      "Training the BPE tokeniser on the full Cosmopedia dataset text...\n",
      "BPE tokeniser model saved to bpe_tokenizer_model\n",
      "Tokenising the entire dataset using the trained BPE tokeniser...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3361287b1164b7cbe6d09c873ce0553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/24123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenised dataset saved to disk as 'tokenized_cosmopediaa11'.\n",
      "Tokenised dataset loaded from disk.\n",
      "Dataset features: {'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saikr\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import AdamW, AutoTokenizer\n",
    "import torch.nn.utils.prune as prune\n",
    "import pennylane as qml\n",
    "\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"PennyLane Version: {qml.__version__}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data Preparation: Load and Process the Dataset\n",
    "# =============================================================================\n",
    "dataset = load_dataset('HuggingFaceTB/cosmopedia', 'khanacademy', \n",
    "                         streaming=True,\n",
    "                         split='train')\n",
    "output_file = \"cosmopedia_full.txt\"\n",
    "example_limit = None  # Set to an integer (e.g. 10000) to limit processing, or None for all examples.\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    count = 0\n",
    "    for example in dataset:\n",
    "        text = example[\"text\"]\n",
    "        # Remove extra newlines and spaces.\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        if text:\n",
    "            f.write(text + \"\\n\")\n",
    "            count += 1\n",
    "        if example_limit is not None and count >= example_limit:\n",
    "            break\n",
    "        if count % 1000 == 0 and count > 0:\n",
    "            print(f\"Processed {count} examples...\")\n",
    "            \n",
    "print(f\"Completed writing {count} examples to {output_file}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Train a BPE Tokeniser on the Entire Cosmopedia Dataset Text\n",
    "# =============================================================================\n",
    "print(\"Training the BPE tokeniser on the full Cosmopedia dataset text...\")\n",
    "bpe_tokenizer = ByteLevelBPETokenizer()\n",
    "bpe_tokenizer.train(\n",
    "    files=output_file,\n",
    "    vocab_size=30000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    ")\n",
    "output_dir = \"bpe_tokenizer_model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "bpe_tokenizer.save_model(output_dir)\n",
    "print(f\"BPE tokeniser model saved to {output_dir}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2.1 Re-map the token IDs to form a contiguous range.\n",
    "#      (The tokeniserâ€™s raw vocabulary IDs might not be in 0..N-1.)\n",
    "# -----------------------------------------------------------------------------\n",
    "vocab_dict = bpe_tokenizer.get_vocab()  # Returns a dict {token: id}\n",
    "# Create a mapping from the original (old) ID to a new contiguous ID.\n",
    "sorted_vocab = sorted(vocab_dict.items(), key=lambda x: x[1])\n",
    "mapping = {old_id: new_id for new_id, (token, old_id) in enumerate(sorted_vocab)}\n",
    "new_vocab_size = len(vocab_dict)  # This is now the actual number of tokens.\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Tokenise the Dataset Using the Trained BPE Tokeniser & Save It\n",
    "# =============================================================================\n",
    "seq_len = 128  # Maximum sequence length\n",
    "\n",
    "def tokenize_and_pad(text):\n",
    "    encoded = bpe_tokenizer.encode(text)\n",
    "    old_ids = encoded.ids\n",
    "    # Remap each token id; if a token is missing in mapping, use the <unk> token.\n",
    "    unk_old = bpe_tokenizer.token_to_id(\"<unk>\")\n",
    "    unk_new = mapping.get(unk_old, 0)\n",
    "    new_ids = [mapping.get(i, unk_new) for i in old_ids]\n",
    "    if len(new_ids) < seq_len:\n",
    "        pad_old = bpe_tokenizer.token_to_id(\"<pad>\")\n",
    "        pad_new = mapping.get(pad_old, 0)\n",
    "        new_ids = new_ids + [pad_new] * (seq_len - len(new_ids))\n",
    "    else:\n",
    "        new_ids = new_ids[:seq_len]\n",
    "    return new_ids\n",
    "\n",
    "print(\"Tokenising the entire dataset using the trained BPE tokeniser...\")\n",
    "tokenized_data = {\"input_ids\": []}\n",
    "with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            token_ids = tokenize_and_pad(line)\n",
    "            tokenized_data[\"input_ids\"].append(token_ids)\n",
    "\n",
    "# Create an attention mask: 1 for non-pad tokens and 0 for pad tokens.\n",
    "pad_old = bpe_tokenizer.token_to_id(\"<pad>\")\n",
    "pad_new = mapping.get(pad_old, 0)\n",
    "attention_masks = []\n",
    "for ids in tokenized_data[\"input_ids\"]:\n",
    "    mask = [1 if token_id != pad_new else 0 for token_id in ids]\n",
    "    attention_masks.append(mask)\n",
    "tokenized_data[\"attention_mask\"] = attention_masks\n",
    "\n",
    "# Convert the dict to a Hugging Face Dataset and save to disk.\n",
    "tokenized_dataset = Dataset.from_dict(tokenized_data)\n",
    "tokenized_dataset.save_to_disk(\"tokenized_cosmopediaa11\")\n",
    "print(\"Tokenised dataset saved to disk as 'tokenized_cosmopediaa11'.\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Load the Tokenised Dataset & Prepare DataLoader for Training\n",
    "# =============================================================================\n",
    "tokenized_dataset = load_from_disk(\"tokenized_cosmopediaa11\")\n",
    "print(\"Tokenised dataset loaded from disk.\")\n",
    "print(\"Dataset features:\", tokenized_dataset.features)\n",
    "\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5.3 Low-Rank Factorisation Feedforward Layer\n",
    "class LowRankLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=128):\n",
    "        super(LowRankLinear, self).__init__()\n",
    "        self.U = nn.Linear(in_features, rank, bias=False)\n",
    "        self.V = nn.Linear(rank, out_features, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.V(self.U(x))\n",
    "\n",
    "\n",
    "\n",
    "# 5.5 Quantisation Wrapper (Placeholder)\n",
    "class QuantizedLayer(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(QuantizedLayer, self).__init__()\n",
    "        self.module = module\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.module(x)\n",
    "\n",
    "# 5.6 Pruning Wrapper (L1 Unstructured Pruning)\n",
    "class PrunedTransformer(nn.Module):\n",
    "    def __init__(self, module, amount=0.3):\n",
    "        super(PrunedTransformer, self).__init__()\n",
    "        self.module = module\n",
    "        for name, child in self.module.named_modules():\n",
    "            if isinstance(child, nn.Linear):\n",
    "                prune.l1_unstructured(child, name=\"weight\", amount=amount)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.module(x)\n",
    "\n",
    "# 5.1 Mixture of Experts (MoE)\n",
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, d_model, num_experts=8, top_k=5):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(num_experts)])\n",
    "        self.gate = nn.Linear(d_model, num_experts)\n",
    "        self.last_gate = None  # Placeholder for analysis\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, d_model]\n",
    "        gate_logits = self.gate(x)  # [batch, seq_len, num_experts]\n",
    "        gate_probs = F.softmax(gate_logits, dim=-1)\n",
    "        self.last_gate = gate_probs  # Save gating probabilities for analysis\n",
    "\n",
    "        topk_probs, topk_indices = torch.topk(gate_probs, self.top_k, dim=-1)\n",
    "        \n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        output = torch.zeros_like(x)\n",
    "        for b in range(batch_size):\n",
    "            for t in range(seq_len):\n",
    "                token = x[b, t]  # [d_model]\n",
    "                expert_sum = 0\n",
    "                for i in range(self.top_k):\n",
    "                    expert_index = topk_indices[b, t, i]\n",
    "                    prob = topk_probs[b, t, i]\n",
    "                    expert_out = self.experts[expert_index](token)\n",
    "                    expert_sum += prob * expert_out\n",
    "                output[b, t] = expert_sum\n",
    "        return output\n",
    "\n",
    "# 5.4 Sparse Self-Attention (Linformer-like) with Causal Masking\n",
    "class LinformerSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, seq_len, compression_ratio=0.7):\n",
    "        super(LinformerSelfAttention, self).__init__()\n",
    "        reduced_dim = int(d_model * compression_ratio)\n",
    "        self.query_proj = nn.Linear(d_model, d_model)\n",
    "        self.key_proj = nn.Linear(d_model, reduced_dim)\n",
    "        self.value_proj = nn.Linear(d_model, reduced_dim)\n",
    "        self.out_proj = nn.Linear(reduced_dim, d_model)\n",
    "        self.last_attn = None  # Placeholder for analysis\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, d_model]\n",
    "        Q = self.query_proj(x)\n",
    "        K = self.key_proj(x)\n",
    "        V = self.value_proj(x)\n",
    "        # Adjust Q to match the dimension of K if necessary.\n",
    "        Q_reduced = Q[:, :, :K.size(-1)]\n",
    "        \n",
    "        # Create a causal mask so that token t only attends to tokens <= t.\n",
    "        seq_length = x.size(1)\n",
    "        mask = torch.tril(torch.ones((seq_length, seq_length), device=x.device))\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, seq_len, seq_len]\n",
    "        \n",
    "        attn_scores = torch.matmul(Q_reduced, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)\n",
    "        attn_scores = attn_scores.masked_fill(mask.squeeze(0) == 0, float('-inf'))\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        self.last_attn = attn_probs  # Save attention weights for analysis\n",
    "        \n",
    "        attn_output = torch.matmul(attn_probs, V)\n",
    "        output = self.out_proj(attn_output)\n",
    "        return output\n",
    "\n",
    "# 5.8 Extend SMARTModel to Build a Language Model (SMARTLMModel) without RAG\n",
    "class SMARTLMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_seq_len, num_experts=8):\n",
    "        super(SMARTLMModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        # Create positional embeddings for the maximum sequence length.\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_seq_len, d_model))\n",
    "        # Removed RAG module from the SMARTModel\n",
    "        self.smart = nn.Sequential(\n",
    "            SparseMoE(d_model, num_experts=num_experts, top_k=5),\n",
    "            LinformerSelfAttention(d_model, seq_len=max_seq_len, compression_ratio=0.7),\n",
    "            # LowRank feedforward network remains as part of SMARTModel.\n",
    "            LowRankLinear(d_model, d_model, rank=min(128, d_model))\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: [batch, L] where L can be less than max_seq_len.\n",
    "        x = self.embed(input_ids)          # [batch, L, d_model]\n",
    "        pos_embed = self.pos_embed[:, :x.size(1), :]  # [1, L, d_model]\n",
    "        x = x + pos_embed\n",
    "        # Pass through the simplified SMARTModel without RAG.\n",
    "        x = self.smart(x)\n",
    "        # Residual connection and normalisation can be applied here as well.\n",
    "        x = self.norm(x)\n",
    "        x = self.norm(x + self.dropout(x))\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Training Setup and Loop\n",
    "# =============================================================================\n",
    "# IMPORTANT: Use the new_vocab_size (the contiguous vocabulary size) for the model.\n",
    "d_model = 512       # Embedding/hidden dimension.\n",
    "seq_len = 128       # Sequence length (should match tokenisation).\n",
    "num_epochs = 20\n",
    "learning_rate = 5e-5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Re-create the DataLoader (if needed)\n",
    "train_dataloader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = SMARTLMModel(vocab_size=new_vocab_size, d_model=d_model, max_seq_len=seq_len, num_experts=8)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Use a Hugging Face tokeniser (e.g. GPT-2) to obtain the pad token for loss computation.\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=hf_tokenizer.pad_token_id if hf_tokenizer.pad_token_id is not None else -100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2409611-903f-4a32-bce1-2fbeaad95a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 5.7727\n",
      "Epoch 2/20 - Loss: 4.6516\n",
      "Epoch 3/20 - Loss: 4.3008\n",
      "Epoch 4/20 - Loss: 4.0676\n",
      "Epoch 5/20 - Loss: 3.8918\n",
      "Epoch 6/20 - Loss: 3.7535\n",
      "Epoch 7/20 - Loss: 3.6347\n",
      "Epoch 8/20 - Loss: 3.5284\n",
      "Epoch 9/20 - Loss: 3.4325\n",
      "Epoch 10/20 - Loss: 3.3447\n",
      "Epoch 11/20 - Loss: 3.2637\n",
      "Epoch 12/20 - Loss: 3.1892\n",
      "Epoch 13/20 - Loss: 3.1194\n",
      "Epoch 14/20 - Loss: 3.0548\n",
      "Epoch 15/20 - Loss: 2.9934\n",
      "Epoch 16/20 - Loss: 2.9359\n",
      "Epoch 17/20 - Loss: 2.8820\n",
      "Epoch 18/20 - Loss: 2.8314\n",
      "Epoch 19/20 - Loss: 2.7837\n",
      "Epoch 20/20 - Loss: 2.7383\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)  # [batch, seq_len]\n",
    "        # Shift tokens for autoregressive prediction:\n",
    "        inputs = input_ids[:, :-1]\n",
    "        targets = input_ids[:, 1:]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)  # logits: [batch, seq_len-1, vocab_size]\n",
    "        loss = loss_fn(logits.reshape(-1, new_vocab_size), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46c1b8ae-10c6-4795-a6c4-0c23c2b648f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SMARTLMModel(\n",
       "  (embed): Embedding(30000, 512)\n",
       "  (smart): SMARTModel(\n",
       "    (moe): SparseMoE(\n",
       "      (experts): ModuleList(\n",
       "        (0-7): 8 x Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (gate): Linear(in_features=512, out_features=8, bias=True)\n",
       "    )\n",
       "    (rag): RAGRetriever()\n",
       "    (attn): LinformerSelfAttention(\n",
       "      (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (key_proj): Linear(in_features=512, out_features=358, bias=True)\n",
       "      (value_proj): Linear(in_features=512, out_features=358, bias=True)\n",
       "      (out_proj): Linear(in_features=358, out_features=512, bias=True)\n",
       "    )\n",
       "    (ffn): LowRankLinear(\n",
       "      (U): Linear(in_features=512, out_features=128, bias=False)\n",
       "      (V): Linear(in_features=128, out_features=512, bias=False)\n",
       "    )\n",
       "    (quantized_ffn): QuantizedLayer(\n",
       "      (module): LowRankLinear(\n",
       "        (U): Linear(in_features=512, out_features=128, bias=False)\n",
       "        (V): Linear(in_features=128, out_features=512, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (pruned_ffn): PrunedTransformer(\n",
       "      (module): QuantizedLayer(\n",
       "        (module): LowRankLinear(\n",
       "          (U): Linear(in_features=512, out_features=128, bias=False)\n",
       "          (V): Linear(in_features=128, out_features=512, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=30000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7. Inference: Generate Output from the Trained Model\n",
    "# =============================================================================\n",
    "# Reload the BPE tokeniser from disk.\n",
    "bpe_tokenizer = ByteLevelBPETokenizer(\n",
    "    os.path.join(\"bpe_tokenizer_model\", \"vocab.json\"),\n",
    "    os.path.join(\"bpe_tokenizer_model\", \"merges.txt\")\n",
    ")\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e189244-9b50-4d32-ba07-5e03d6b951bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inference Example ---\n",
      "Prompt: A number grid is a simple yet powerful tool for helping students \n",
      "Generated Output: A number grid is a simple yet powerful tool for helping students  and understanding of data. It is important to understand the relationship between two variables and variables. In this section, we will explore how to interpret the relationship between two variables and variables. Let's start with a simple example: Suppose we want to find the\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, model, bpe_tokenizer, mapping, max_seq_len=128, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Greedy decoding with a sliding window.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    def tokenize_prompt(text):\n",
    "        encoded = bpe_tokenizer.encode(text)\n",
    "        old_ids = encoded.ids\n",
    "        unk_old = bpe_tokenizer.token_to_id(\"<unk>\")\n",
    "        unk_new = mapping.get(unk_old, 0)\n",
    "        new_ids = [mapping.get(i, unk_new) for i in old_ids]\n",
    "        return torch.tensor(new_ids).unsqueeze(0)  # Shape: [1, L]\n",
    "    \n",
    "    generated = tokenize_prompt(prompt).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            if generated.size(1) > max_seq_len:\n",
    "                current_input = generated[:, -max_seq_len:]\n",
    "            else:\n",
    "                current_input = generated\n",
    "            logits = model(current_input)  # [1, current_length, vocab_size]\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits).unsqueeze(0).unsqueeze(0)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            # Stop if the end-of-sequence token is generated.\n",
    "            if next_token.item() == mapping.get(bpe_tokenizer.token_to_id(\"</s>\"), -1):\n",
    "                break\n",
    "    \n",
    "    generated_ids = generated[0].cpu().tolist()\n",
    "    decoded_text = bpe_tokenizer.decode(generated_ids)\n",
    "    return decoded_text\n",
    "\n",
    "# Example inference:\n",
    "prompt_text = \"A number grid is a simple yet powerful tool for helping students \"\n",
    "generated_output = generate_text(prompt_text, model, bpe_tokenizer, mapping, max_seq_len=128, max_new_tokens=50)\n",
    "print(\"\\n--- Inference Example ---\")\n",
    "print(\"Prompt:\", prompt_text)\n",
    "print(\"Generated Output:\", generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f551d9c8-8f50-468a-bd85-8023c44de351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d18251c0-5c49-4b68-b578-306d86269cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inference Example ---\n",
      "Prompt: Counting by Tens\n",
      "Generated Output: Counting by Tens in the United States, and their role in the United States. The Constitution was the United States of the United States Constitution in the United States, which had been established by the Soviet Union of the U.S. Constitution, the federal government, government, and government, and the government, and government, and government. One significant consequences for the government was the government in the federal government, the Constitution, which states that the Constitution was the Constitution was the Constitution in the President of the Constitution, which\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, model, bpe_tokenizer, mapping, max_seq_len=128, max_new_tokens=100):\n",
    "    \"\"\"\n",
    "    Greedy decoding with a sliding window.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    def tokenize_prompt(text):\n",
    "        encoded = bpe_tokenizer.encode(text)\n",
    "        old_ids = encoded.ids\n",
    "        unk_old = bpe_tokenizer.token_to_id(\"<unk>\")\n",
    "        unk_new = mapping.get(unk_old, 0)\n",
    "        new_ids = [mapping.get(i, unk_new) for i in old_ids]\n",
    "        return torch.tensor(new_ids).unsqueeze(0)  # Shape: [1, L]\n",
    "    \n",
    "    generated = tokenize_prompt(prompt).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            if generated.size(1) > max_seq_len:\n",
    "                current_input = generated[:, -max_seq_len:]\n",
    "            else:\n",
    "                current_input = generated\n",
    "            logits = model(current_input)  # [1, current_length, vocab_size]\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits).unsqueeze(0).unsqueeze(0)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            # Stop if the end-of-sequence token is generated.\n",
    "            if next_token.item() == mapping.get(bpe_tokenizer.token_to_id(\"</s>\"), -1):\n",
    "                break\n",
    "    \n",
    "    generated_ids = generated[0].cpu().tolist()\n",
    "    decoded_text = bpe_tokenizer.decode(generated_ids)\n",
    "    return decoded_text\n",
    "\n",
    "# Example inference:\n",
    "prompt_text = \"Counting by Tens\"\n",
    "generated_output = generate_text(prompt_text, model, bpe_tokenizer, mapping, max_seq_len=128, max_new_tokens=100)\n",
    "print(\"\\n--- Inference Example ---\")\n",
    "print(\"Prompt:\", prompt_text)\n",
    "print(\"Generated Output:\", generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fecb1c-9a78-4329-a284-e866c1910cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2373453c-75b0-46f4-b6f2-acfbe7c75c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f26228-efa8-46d5-92df-61ecbeef1b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bb0e0b-fcc3-494f-a7a9-133f30b8222a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb6a334-e97c-487a-8e78-452fbc8efbbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a06568b-ad16-4bd6-9fe6-4063c6604f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af9f2ad-370f-4841-b8a6-8bbb2187c7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuda_env)",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
